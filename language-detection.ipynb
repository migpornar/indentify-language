{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trabajo PLN I. Detección de idiomas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autores:\n",
    "\n",
    "   **- Porras Naranjo, Miguel Ángel**\n",
    "   \n",
    "   **- Terrón Dastis, Pedro**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se propone a continuación un modelo de clasificador de texto en lenguaje natural cuyo objetivo es determinar el idioma de dicho texto.\n",
    "\n",
    "En primer lugar, se procede a importar una serie de corpus en distintos idiomas que pueden encontrarse en el siguiente enlace: http://www.statmt.org/europarl/\n",
    "\n",
    "Los idiomas tratados (9 en total) son los siguientes:\n",
    "    - Inglés\n",
    "    - Español\n",
    "    - Francés\n",
    "    - Italiano\n",
    "    - Alemán\n",
    "    - Portugués\n",
    "    - Griego\n",
    "    - Húngaro\n",
    "    - Polaco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importamos el corpus en inglés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('europarl-v7.es-en.en', encoding = \"utf-8\")\n",
    "raw_en = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Importamos el corpus en español."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('europarl-v7.es-en.es', encoding = \"utf-8\")\n",
    "raw_sp = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importamos el corpus en francés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('europarl-v7.fr-en.fr', encoding = \"utf-8\")\n",
    "raw_fr = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importamos el corpus en italiano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('europarl-v7.it-en.it', encoding = \"utf-8\")\n",
    "raw_it = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importamos el corpus en alemán."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('europarl-v7.de-en.de', encoding = \"utf-8\")\n",
    "raw_ger = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importamos el corpus en portugués."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('europarl-v7.pt-en.pt', encoding = \"utf-8\")\n",
    "raw_por = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importamos el corpus en griego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('europarl-v7.el-en.el', encoding = \"utf-8\")\n",
    "raw_gr = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importamos el corpus en húngaro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('europarl-v7.hu-en.hu', encoding = \"utf-8\")\n",
    "raw_hu = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importamos el corpus en polaco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('europarl-v7.pl-en.pl', encoding = \"utf-8\")\n",
    "raw_pol = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Indicamos cuántos elementos queremos seleccionar de cada corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separación de los corpus en frases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mediante el siguiente fragmento de código se separan los textos importados en frases completas mediante la función sent_tokenize de la librería NLTK. \n",
    "\n",
    "Se eliminan asimismo los caracteres \"\\n\" correspondientes a los saltos de línea mediante expresiones regulares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sents_en = sent_tokenize(raw_en[:n])\n",
    "sents_sp = sent_tokenize(raw_sp[:n])\n",
    "sents_fr = sent_tokenize(raw_fr[:n])\n",
    "sents_it = sent_tokenize(raw_it[:n])\n",
    "sents_ger = sent_tokenize(raw_ger[:n])\n",
    "sents_por = sent_tokenize(raw_por[:n])\n",
    "sents_gr = sent_tokenize(raw_gr[:n])\n",
    "sents_hu = sent_tokenize(raw_hu[:n])\n",
    "sents_pol = sent_tokenize(raw_pol[:n])\n",
    "\n",
    "\n",
    "sents_en2=[]\n",
    "for i in sents_en:\n",
    "    sents_en2.append(re.sub(r\"\\n\",\" \",i))\n",
    "    \n",
    "sents_sp2=[]\n",
    "for i in sents_sp:\n",
    "    sents_sp2.append(re.sub(r\"\\n\",\" \",i))    \n",
    "\n",
    "sents_fr2=[]\n",
    "for i in sents_fr:\n",
    "    sents_fr2.append(re.sub(r\"\\n\",\" \",i))\n",
    "    \n",
    "sents_it2=[]\n",
    "for i in sents_it:\n",
    "    sents_it2.append(re.sub(r\"\\n\",\" \",i))\n",
    "    \n",
    "sents_ger2=[]\n",
    "for i in sents_ger:\n",
    "    sents_ger2.append(re.sub(r\"\\n\",\" \",i))\n",
    "    \n",
    "sents_por2=[]\n",
    "for i in sents_por:\n",
    "    sents_por2.append(re.sub(r\"\\n\",\" \",i))\n",
    "    \n",
    "sents_gr2=[]\n",
    "for i in sents_gr:\n",
    "    sents_gr2.append(re.sub(r\"\\n\",\" \",i))\n",
    "    \n",
    "sents_hu2=[]\n",
    "for i in sents_hu:\n",
    "    sents_hu2.append(re.sub(r\"\\n\",\" \",i))\n",
    "    \n",
    "sents_pol2=[]\n",
    "for i in sents_pol:\n",
    "    sents_pol2.append(re.sub(r\"\\n\",\" \",i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separación de las frases de los corpus en palabras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mediante la función word_tokenize de la librería NLTK se dividen los corpus completos en palabras, de modo que éstas puedan ser etiquetadas con el idioma del corpus al que pertenecen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words_en2=[]\n",
    "for i in sents_en2:\n",
    "    words_en2.append((word_tokenize(i), \"Inglés\"))\n",
    "    \n",
    "words_sp2=[]\n",
    "for i in sents_sp2:\n",
    "    words_sp2.append((word_tokenize(i), \"Español\"))    \n",
    "\n",
    "words_fr2=[]\n",
    "for i in sents_fr2:\n",
    "    words_fr2.append((word_tokenize(i), \"Francés\"))\n",
    "    \n",
    "words_it2=[]\n",
    "for i in sents_it2:\n",
    "    words_it2.append((word_tokenize(i), \"Italiano\"))\n",
    "    \n",
    "words_ger2=[]\n",
    "for i in sents_ger2:\n",
    "    words_ger2.append((word_tokenize(i), \"Alemán\"))\n",
    "    \n",
    "words_por2=[]\n",
    "for i in sents_por2:\n",
    "    words_por2.append((word_tokenize(i), \"Portugués\"))\n",
    "    \n",
    "words_gr2=[]\n",
    "for i in sents_gr2:\n",
    "    words_gr2.append((word_tokenize(i), \"Griego\"))\n",
    "    \n",
    "words_hu2=[]\n",
    "for i in sents_hu2:\n",
    "    words_hu2.append((word_tokenize(i), \"Húngaro\"))\n",
    "    \n",
    "words_pol2=[]\n",
    "for i in sents_pol2:\n",
    "    words_pol2.append((word_tokenize(i), \"Polaco\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unificación de los corpus en todos los idiomas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se unifican todos los corpus, que han sido previamente separados por palabras etiquetadas según su idioma.\n",
    "\n",
    "Con la función shuffle de la librería random se disponen de forma aleatoria todos los elementos del corpus unificado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aux = [words_en2, words_sp2, words_fr2, words_it2, words_ger2, words_por2, \n",
    "       words_gr2, words_hu2, words_pol2]\n",
    "\n",
    "corp_total = []\n",
    "for i in aux:\n",
    "    for j in i:\n",
    "        corp_total.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random.shuffle(corp_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creación de una lista con todas las palabras del corpus completo normalizadas para que estén en minúsculas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el siguiente fragmento de código se pretende crear una lista con todas las palabras del corpus completo sin etiquetar.\n",
    "\n",
    "Se le ha aplicado a cada una de las palabras la función \"lower()\" con el objetivo de normalizar el texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words_en = word_tokenize(raw_en[:n])\n",
    "words_sp = word_tokenize(raw_sp[:n])\n",
    "words_fr = word_tokenize(raw_fr[:n])\n",
    "words_it = word_tokenize(raw_it[:n])\n",
    "words_ger = word_tokenize(raw_ger[:n])\n",
    "words_por = word_tokenize(raw_por[:n])\n",
    "words_gr = word_tokenize(raw_gr[:n])\n",
    "words_hu = word_tokenize(raw_hu[:n])\n",
    "words_pol = word_tokenize(raw_pol[:n])\n",
    "lan_list = words_en + words_sp + words_fr + words_it + words_ger + words_por + words_gr + words_hu + words_pol\n",
    "\n",
    "all_words = []\n",
    "for w in lan_list:\n",
    "    all_words.append(w.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creación de una función para definir las variables dummies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la variable \"featuresets\" se va a almacenar el conjunto de variables dummies generadas para poder llevar a cabo la tarea de clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_features(doc):\n",
    "    words = set(doc)\n",
    "    features = {}\n",
    "    for w in all_words:\n",
    "        features[w] = (w in words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featureset = [(find_features(phrase),language) for (phrase,language) in corp_total]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicación del modelo de clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5437"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(featureset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5437"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corp_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creación de los subconjuntos de entrenamiento y de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_set = featureset[:round(0.8*len(featureset))]\n",
    "test_set = featureset[round(0.8*len(featureset))+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#len(training_set)\n",
    "#len(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naïve-Bayes Classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se aplica en primer lugar el clasificador NaiveBayesClassifier incluido en la librería NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(training_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mediante la siguiente instrucción se imprime por pantalla la eficiencia del clasificador generado cuando predice los casos contenidos en el subconjunto de prueba (test_set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Algorithm accuracy percent: 95.85635359116023\n"
     ]
    }
   ],
   "source": [
    "print(\"Naive Bayes Algorithm accuracy percent:\", (nltk.classify.accuracy(classifier, test_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                       a = True           Portug : Alemán =    197.1 : 1.0\n",
      "                      et = True           Francé : Polaco =    186.6 : 1.0\n",
      "                      le = True           Francé : Polaco =    172.1 : 1.0\n",
      "                       e = True           Italia : Húngar =    153.4 : 1.0\n",
      "                      de = True           Españo : Inglés =    152.7 : 1.0\n",
      "                     los = True           Españo : Polaco =    144.1 : 1.0\n",
      "                     der = True           Alemán : Polaco =    117.0 : 1.0\n",
      "                       i = True           Polaco : Alemán =    116.5 : 1.0\n",
      "                       y = True           Españo : Polaco =    100.7 : 1.0\n",
      "                      of = True           Inglés : Polaco =     96.5 : 1.0\n",
      "                     del = True           Españo : Polaco =     95.6 : 1.0\n",
      "                      zu = True           Alemán : Polaco =     86.3 : 1.0\n",
      "                    dans = True           Francé : Húngar =     85.1 : 1.0\n",
      "                    pour = True           Francé : Polaco =     84.6 : 1.0\n",
      "                      en = True           Españo : Polaco =     82.5 : 1.0\n",
      "                     die = True           Alemán : Polaco =     78.5 : 1.0\n",
      "                     für = True           Alemán : Polaco =     76.8 : 1.0\n",
      "                      je = True           Francé : Polaco =     76.1 : 1.0\n",
      "                      es = True           Españo : Húngar =     72.6 : 1.0\n",
      "                      il = True           Italia : Húngar =     71.5 : 1.0\n",
      "                      di = True           Italia : Húngar =     67.3 : 1.0\n",
      "                     con = True           Españo : Polaco =     65.9 : 1.0\n",
      "                       z = True           Polaco : Alemán =     64.6 : 1.0\n",
      "                       é = True           Portug : Polaco =     64.0 : 1.0\n",
      "                      da = True           Portug : Españo =     63.3 : 1.0\n",
      "                     dos = True           Portug : Francé =     62.5 : 1.0\n",
      "                     les = True           Francé : Polaco =     61.4 : 1.0\n",
      "                      on = True           Inglés : Italia =     60.9 : 1.0\n",
      "                      ce = True           Francé : Italia =     55.5 : 1.0\n",
      "                      al = True           Españo : Polaco =     54.8 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matriz de confusión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La matriz de confusión permite determinar la bondad del clasificador para cada una de las categorías."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.metrics import ConfusionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          |                                   P |\n",
      "          |                           I       o |\n",
      "          |       E   F       H       t       r |\n",
      "          |   A   s   r   G   ú   I   a   P   t |\n",
      "          |   l   p   a   r   n   n   l   o   u |\n",
      "          |   e   a   n   i   g   g   i   l   g |\n",
      "          |   m   ñ   c   e   a   l   a   a   u |\n",
      "          |   á   o   é   g   r   é   n   c   é |\n",
      "          |   n   l   s   o   o   s   o   o   s |\n",
      "----------+-------------------------------------+\n",
      "   Alemán |<131>  .   .   .   .   .   .   8   . |\n",
      "  Español |   2<112>  .   .   .   .   .   5   . |\n",
      "  Francés |   .   . <88>  .   1   .   .   6   . |\n",
      "   Griego |   .   .   .<104>  .   .   .   3   . |\n",
      "  Húngaro |   1   .   .   .<138>  .   .   7   . |\n",
      "   Inglés |   .   .   .   .   .<117>  .   2   . |\n",
      " Italiano |   .   .   .   .   1   . <99>  2   . |\n",
      "   Polaco |   .   .   .   .   .   .   .<142>  . |\n",
      "Portugués |   1   .   .   .   1   .   .   5<109>|\n",
      "----------+-------------------------------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ref = [] #lista con los valores etiquetados\n",
    "tagged = [] #lista con los valores predichos\n",
    "\n",
    "for i in range(0,len(test_set)-1):\n",
    "    ref.append(test_set[i][1])\n",
    "\n",
    "for i in range(len(training_set)+1,len(training_set)+len(test_set)):\n",
    "    tagged.append(classifier.classify(find_features(corp_total[i][0])))\n",
    "    \n",
    "cm = ConfusionMatrix(ref, tagged)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este primer modelo puede verse que el polaco es el idioma que más errores presenta. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probando con otros clasificadores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naïve-Bayes Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SklearnClassifier(MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "MNB_classifier.train(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNB_classifier accuracy percent: 98.34254143646409\n"
     ]
    }
   ],
   "source": [
    "print(\"MNB_classifier accuracy percent:\", (nltk.classify.accuracy(MNB_classifier, test_set))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matriz de confusión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          |                                   P |\n",
      "          |                           I       o |\n",
      "          |       E   F       H       t       r |\n",
      "          |   A   s   r   G   ú   I   a   P   t |\n",
      "          |   l   p   a   r   n   n   l   o   u |\n",
      "          |   e   a   n   i   g   g   i   l   g |\n",
      "          |   m   ñ   c   e   a   l   a   a   u |\n",
      "          |   á   o   é   g   r   é   n   c   é |\n",
      "          |   n   l   s   o   o   s   o   o   s |\n",
      "----------+-------------------------------------+\n",
      "   Alemán |<137>  .   .   .   .   .   .   2   . |\n",
      "  Español |   .<117>  .   .   1   .   .   1   . |\n",
      "  Francés |   .   . <93>  .   .   1   .   1   . |\n",
      "   Griego |   .   .   .<107>  .   .   .   .   . |\n",
      "  Húngaro |   1   .   .   .<140>  1   .   4   . |\n",
      "   Inglés |   .   .   .   .   .<119>  .   .   . |\n",
      " Italiano |   .   .   .   .   .   .<101>  1   . |\n",
      "   Polaco |   3   .   .   .   1   .   .<138>  . |\n",
      "Portugués |   .   .   .   .   .   .   .   1<115>|\n",
      "----------+-------------------------------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ref = [] #lista con los valores etiquetados\n",
    "tagged = [] #lista con los valores predichos\n",
    "\n",
    "for i in range(0,len(test_set)-1):\n",
    "    ref.append(test_set[i][1])\n",
    "\n",
    "for i in range(len(training_set)+1,len(training_set)+len(test_set)):\n",
    "    tagged.append(MNB_classifier.classify(find_features(corp_total[i][0])))\n",
    "    \n",
    "cm = ConfusionMatrix(ref, tagged)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se selecciona, por ser más eficiente, el clasificador multinomial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueba del clasificador con ejemplos concretos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Español'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MNB_classifier.classify(find_features(word_tokenize(\"los informes de la junta\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ochrona',\n",
       " 'karna',\n",
       " 'w',\n",
       " 'zakresie',\n",
       " 'ochrony',\n",
       " 'środowiska',\n",
       " '(',\n",
       " 'głosowanie',\n",
       " ')',\n",
       " '10',\n",
       " '.']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corp_total[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Polaco'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MNB_classifier.classify(find_features(corp_total[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alemán',\n",
       " 'Español',\n",
       " 'Francés',\n",
       " 'Griego',\n",
       " 'Húngaro',\n",
       " 'Inglés',\n",
       " 'Italiano',\n",
       " 'Polaco',\n",
       " 'Portugués']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MNB_classifier.labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
